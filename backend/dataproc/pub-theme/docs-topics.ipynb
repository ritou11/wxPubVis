{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "\n",
    "import jieba\n",
    "\n",
    "def load_stopwords():\n",
    "    f_stop = open('stop_words.txt', 'r')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    return sw\n",
    "\n",
    "#分词+过滤停用词\n",
    "\n",
    "def word_cut(text):\n",
    "    text = str(text)\n",
    "    seg = jieba.cut(text.strip())\n",
    "    outstr = \"\"\n",
    "    for word in seg:  \n",
    "            if word not in stopwords:  \n",
    "                if word != '\\t':  \n",
    "                    outstr += word  \n",
    "                    outstr += \" \"  \n",
    "    return outstr\n",
    "\n",
    "#打印每个主题的前50个相关词\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" %topic_idx)\n",
    "        print(\" \".join([str(feature_names[i])\n",
    "                           for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "#文章主题权重\n",
    "def doc_top(model, tf):\n",
    "    docres = model.fit_transform(tf)\n",
    "    return docres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "#连接数据库\n",
    "conn = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = conn.wechat_spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pstcol = db.posts\n",
    "prfcol = db.profiles\n",
    "\n",
    "def extractPubPosts(msg):\n",
    "\n",
    "    pid = []\n",
    "    pubname = []\n",
    "    tit = []\n",
    "    dig = []\n",
    "    con = []\n",
    "    readNum = []\n",
    "\n",
    "    print(msg['msgBiz'])\n",
    "    \n",
    "    pstcursor = pstcol.find(msg)\n",
    "    \n",
    "    prfcursor = prfcol.find()\n",
    "    \n",
    "    for i, s in enumerate(pstcursor):\n",
    "        if 'content' in s:\n",
    "            for idx, pn in enumerate(prfcursor):\n",
    "                print(idx)\n",
    "                print(str(pn['msgBiz']))\n",
    "                if msg['msgBiz'] == str(pn['msgBiz']):\n",
    "                    pname = pn['title']\n",
    "            pubname.append(str(pname))\n",
    "            pid.append(str(s['_id']))\n",
    "            tit.append(str(s['title']))\n",
    "            dig.append(str(s['digest']))\n",
    "            con.append(str(s['content']))\n",
    "            if 'readNum' in s:\n",
    "                readNum.append(str(s['readNum']))\n",
    "            else:\n",
    "                readNum.append(0)\n",
    "    dic = {\"pid\":pid,\n",
    "            \"pubname\":pubname,\n",
    "            \"title\":tit,\n",
    "           \"digest\":dig,\n",
    "           \"content\":con,\n",
    "          \"readNum\":readNum}\n",
    "\n",
    "    df = pd.DataFrame(dic)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theme:{msgBiz, theme, weight}\n",
    "#post:{msgBiz, pid, theme:[name, weight, contrib]}\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "post  = db.post\n",
    "\n",
    "theme = db.theme\n",
    "\n",
    "prfcursor = prfcol.find()\n",
    "\n",
    "for num, pn in enumerate(prfcursor):\n",
    "    \n",
    "    print(num)\n",
    "    \n",
    "    msg = {\"msgBiz\":str(pn['msgBiz'])}\n",
    "    \n",
    "    df = extractPubPosts(msg)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    con = df['title'] + df['content']\n",
    "\n",
    "    df[\"con\"] = con\n",
    "\n",
    "    df[\"con_cutted\"] = df.con.apply(word_cut)\n",
    "\n",
    "    n_features  = 1000\n",
    "    n_topics = 30\n",
    "    n_top_words = 50\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_features = n_features,\n",
    "                                                 stop_words = 'english',\n",
    "                                                 max_df = 0.4,\n",
    "                                                 min_df = 10 )\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(df.con_cutted)\n",
    "\n",
    "    lda = LatentDirichletAllocation(learning_method='online', \n",
    "                                              n_topics=n_topics,\n",
    "                                              perp_tol= 0.001,\n",
    "                                              doc_topic_prior=0.001,\n",
    "                                              topic_word_prior=0.001,\n",
    "                                              max_iter=300)\n",
    "    lda.fit(tf)\n",
    "\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print(\"主题-相关词\")\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)\n",
    "    print ('\\n')\n",
    "    print ('\\n')\n",
    "    print(\"文章-主题权重\")\n",
    "    docres =  doc_top(lda,tf)\n",
    "    print (docres)\n",
    "    print(\"\\n文章-主题贡献\")\n",
    "    readn = df['readNum']\n",
    "    readnum = np.array(df['readNum']).reshape(len(readn),1)\n",
    "    readnum = readnum.repeat(30,axis = 1)\n",
    "    contrib = np.multiply(docres, readnum)\n",
    "    print(contrib)\n",
    "    for idx in range(0, len(df)):\n",
    "        post_dict = {}\n",
    "        post_dict['msgBiz'] = str(pn['msgBiz'])\n",
    "        post_dict['pid'] = str(df['pid'][idx])\n",
    "        for j in range(0,n_topics):\n",
    "            if post_dict.has_key('theme'):\n",
    "                post_dict['theme'].append({'name':str(\"主题\"+str(j+1)), 'weight':docres[idx][j], 'contrib':contrib[idx][j]}) \n",
    "            else:\n",
    "                post_dict['theme'] = [{'name':str(\"主题\"+str(j+1)), 'weight':docres[idx][j], 'contrib':contrib[idx][j]}]\n",
    "        print(post_dict)\n",
    "        result = post.insert_one(post_dict)\n",
    "        print(result)\n",
    "    top_dict = []\n",
    "    for idx in range(0,n_topics):\n",
    "        sum_contrib = 0\n",
    "        for j in range(0, len(df)):\n",
    "            sum_contrib += contrib[j][idx]\n",
    "        top_dict.append({'msgBiz': str(pn['msgBiz']), 'name':str(\"主题\"+str(idx+1)), 'importance':str(sum_contrib)})\n",
    "    print(top_dict)\n",
    "    result = theme.insert_many(top_dict)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
