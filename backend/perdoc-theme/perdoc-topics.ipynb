{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.core.frame import DataFrame\n",
    "import fileinput\n",
    "\n",
    "#存储我们需要的字段到csv文件中\n",
    "pid = []\n",
    "pubname = []\n",
    "tit = []\n",
    "dig = []\n",
    "con = []\n",
    "readNum = []\n",
    "for line in fileinput.FileInput(\"wxPubAnal.json/posts.json\",openhook=fileinput.hook_encoded(\"utf-8\")):\n",
    "    s = json.loads(line)\n",
    "    if s.has_key('content'):\n",
    "        if s['msgBiz'] ==\"MjM5MDc0NTY2OA==\":\n",
    "            pubname.append('洞见')\n",
    "        elif s['msgBiz'] == \"MzUxODM4OTYzMg==\":\n",
    "            pubname.append('清华小五爷园')\n",
    "        elif s['msgBiz'] == \"MzA4MjEyNTA5Mw==\":\n",
    "            pubname.append('Python开发者')\n",
    "        elif s['msgBiz'] == \"MzI1NDY5NDM3OQ==\":\n",
    "            pubname.append('凤凰WEEKLY')\n",
    "        else:\n",
    "            pubname.append('沃顿商业')\n",
    "        pid.append(s['_id']['$oid'])\n",
    "        tit.append(s['title'])\n",
    "        dig.append(s['digest'])\n",
    "        con.append(s['content'])\n",
    "        if s.has_key('readNum'):\n",
    "            readNum.append(s['readNum'])\n",
    "        else:\n",
    "            readNum.append(0)\n",
    "        \n",
    "dic = {\"pid\":pid,\n",
    "        \"pubname\":pubname,\n",
    "        \"title\":tit,\n",
    "       \"digest\":dig,\n",
    "       \"content\":con,\n",
    "      \"readNum\":readNum}\n",
    "df = DataFrame(dic)\n",
    "df.to_csv(r'wxPubAnal.json/posts.csv',columns=['pid','pubname','title','digest','content','readNum'],index=False,sep=',', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')   \n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "import os\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "df = pd.read_csv(\"wxPubAnal.json/posts.csv\", encoding = \"utf-8\")\n",
    "\n",
    "con = df['title'].str.cat(df['content'])\n",
    "\n",
    "con.head()\n",
    "\n",
    "def load_stopwords():\n",
    "    f_stop = open('stop_words.txt', 'r')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    return sw\n",
    "\n",
    "#分词+过滤停用词\n",
    "\n",
    "def word_cut(text):\n",
    "    text = str(text)\n",
    "    seg = jieba.cut(text.strip())\n",
    "    outstr = \"\"\n",
    "    for word in seg:  \n",
    "            if word not in stopwords:  \n",
    "                if word != '\\t':  \n",
    "                    outstr += word  \n",
    "                    outstr += \" \"  \n",
    "    return outstr\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "df[\"con\"] = con\n",
    "\n",
    "df[\"con_cutted\"] = df.con.apply(word_cut)\n",
    "\n",
    "df.con_cutted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#每一个单词关联一个唯一的ID\n",
    "docs = [ [word for word in df.con_cutted[i].split() ] for i in range(0,len(df)) ]\n",
    "word_count_dict = Dictionary(docs)\n",
    "#过滤高频低频词\n",
    "word_count_dict.filter_extremes(no_below=5, no_above=0.5) \n",
    "#将文档表示成词袋向量\n",
    "bag_of_words_corpus = [word_count_dict.doc2bow(perdoc) for perdoc in docs] \n",
    "\n",
    "#保存模型\n",
    "model_name = \"./model.lda\"  \n",
    "if os.path.exists(model_name):\n",
    "    lda_model = gensim.models.LdaModel.load(model_name)  \n",
    "    print(\"loaded from old\")\n",
    "else:\n",
    "    # preprocess()  第一个参数为选用的文档向量，num_topics为主题个数，id2word可选是选用的字典，\n",
    "    lda_model = gensim.models.LdaModel(bag_of_words_corpus, num_topics=80, id2word=word_count_dict)\n",
    "    lda_model.save(model_name)  \n",
    "    print(\"loaded from new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "#结果写入数据库\n",
    "conn = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = conn.wxPub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{pid:pid, theme: “主题1”, words: [{name:”xxx”, freq:123}], weight}\n",
    "\n",
    "perdoc = db.perdoc\n",
    "topic_num=3\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    doc = [ word for word in df.con_cutted[i].split() ]\n",
    "    doc_dict = word_count_dict.doc2bow(doc)\n",
    "    result=lda_model[doc_dict]\n",
    "    result=sorted(result,key=lambda tup: -1 * tup[1])#排序，只取前三的主题\n",
    "    idx = 1\n",
    "    for topic in result:\n",
    "        if idx>topic_num:\n",
    "            break\n",
    "        #print_topic(x,y) x是主题的id，y是打印该主题的前y个词，词是按权重排好序的\n",
    "        dict = {}\n",
    "        dict['pid'] = str(df['pid'][i])\n",
    "        print(\"文章\"+str(i+1))\n",
    "        dict['theme'] = str(\"主题\"+str(idx))\n",
    "        print(\"主题\"+str(idx))\n",
    "        dict['weight'] = str(topic[1])\n",
    "        s = str(lda_model.print_topic(topic[0], 30))\n",
    "        freq_word = s.split(\" + \")\n",
    "        for j in range(0,len(freq_word)):\n",
    "            fw = freq_word[j].split(\"*\")\n",
    "            if dict.has_key('words'):\n",
    "                dict['words'].append({'name':str(fw[1]), 'freq':fw[0]})\n",
    "            else:\n",
    "                dict['words'] = [{'name':str(fw[1]), 'freq':fw[0]}]\n",
    "        idx += 1\n",
    "        #print(dict)\n",
    "        result = perdoc.insert(dict)\n",
    "        #print(result)\n",
    "#print(db.perdoc.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{pid:待查询文章id, sid:匹配文章id, similarity:pid和sid两篇文章相似度}\n",
    "#每篇文章的相似列表\n",
    "sim = db.sim\n",
    "\n",
    "from gensim import corpora, similarities\n",
    "import traceback\n",
    "# 用bag_of_words_corpus作为特征，训练tf_idf_model\n",
    "tf_idf_model = gensim.models.TfidfModel(bag_of_words_corpus)\n",
    "\n",
    "# 每篇文章在vsm上的tf-idf表示\n",
    "corpus_tfidf = tf_idf_model[bag_of_words_corpus]\n",
    "\n",
    "def similarity( i, query, dictionary, corpus_tfidf ):\n",
    "    try:\n",
    "\n",
    "        # 建立索引\n",
    "        index = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "\n",
    "        # 在dictionary建立query的vsm_tf表示\n",
    "        query_bow = dictionary.doc2bow( query.lower().split() )\n",
    "\n",
    "        # 查询在n_topics维空间的表示\n",
    "        query_lda = tf_idf_model[query_bow]\n",
    "\n",
    "        # 计算相似度\n",
    "        simi = index[query_lda]\n",
    "        query_simi_list = []\n",
    "        for idx, item in enumerate(simi):\n",
    "            query_simi_list.append( {'pid':str(df['pid'][i]), 'sid':str(df['pid'][idx]), 'similarity':str(item)} )\n",
    "        result = sim.insert(query_simi_list)\n",
    "        \n",
    "    except Exception,e:\n",
    "        print traceback.print_exc()\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    print(\"相似权重矩阵：\")\n",
    "    similarity(i, str(df.con_cutted[i]), word_count_dict, corpus_tfidf )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
